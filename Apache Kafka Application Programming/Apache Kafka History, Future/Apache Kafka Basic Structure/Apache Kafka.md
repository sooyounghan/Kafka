-----
### 아파치 카프카 탄생
-----
1. 2011년, 구인 / 구직 및 동종업계 동향을 살펴볼 수 있는 소셜 네트워크 사이트인 '링크드인(LinkedIn)'에서 파편화된 데이터 수집 및 분배 아키텍쳐를 운영하는데 큰 어려움을 겪음
   - 데이터를 생성하고 적재하기 위해서는 데이터를 생성하는 소스 애플리케이션과 데이터가 최종 적재되는 타깃 애플리케이션을 연결해야 함
   - 초기 운영 시에는 단방향 통신을 통해 소스 애플리케이션에서 타깃 애플리케이션으로 연동하는 소스코드를 작성했고, 아키텍쳐가 복잡하지 않아 운영이 힘들지 않았음
   - 그러나 시간이 지날수록 아키텍쳐는 거대해졌고, 소스 애플리케이션과 타깃 애플리케이션 개수가 점점 많아지면서 문제가 발생 : 데이터를 전송하는 라인이 기하급수적으로 복잡해지기 시작
<div align="center">
<img src="https://github.com/user-attachments/assets/c1d14ea6-f71c-4baa-84aa-387808e38b17" />
</div>

2. 이를 해결하기 위해 링크드인 데이터팀에서는 기존에 나와 있던 각종 상용 데이터 프레임워크와 오픈소스를 아키텍쳐에 녹여내어 데이터 파이프라인의 파편화를 개선하려고 함
   - 다양한 메세징 플랫폼과 ETL(Extract Transfrom Load) 툴을 적용해 아키텍쳐를 변경하려고 노력했지만, 파편화된 데이터 파이프라인의 복잡도를 낮춰주는 아키텍쳐가 되지 못함
<div align="center">
<img src="https://github.com/user-attachments/assets/6d7c07f1-aaba-436f-a3cc-33a1d5e744c6" />
</div>

3. 결국, 링크드인의 데이터팀은 신규 시스템을 만들기로 결정했고, 그 결과물이 바로 아파치 카프카 (Apache Kakfa)
   - Unified Log라는 중앙에 모으는 로그라는 특징을 가
   - 링크드인 내부 데이터 흐름을 개선하기 위해 개발한 카프카는 매우 훌륭하게 동작
   - 카프카는 각각의 애플리케이션끼리 연결해 데이터를 처리하는 것이 아닌 한 곳에 모아 처리할 수 있도록 중앙 집중화

-----
### 메세지 큐 구조를 그대로 살린 카프카 내부 구조
-----
<div align="center">
<img src="https://github.com/user-attachments/assets/ae70f94c-5556-4897-99c8-e3b8a1a7c64e" />
</div>

1. 기존의 1:1 매칭으로 개발하고 운영하던 데이터 파이프라인은 커플링으로 인해 한쪽의 이슈가 다른 한쪽의 애플리케이션에 영향을 미치곤 했지만, 카프카는 이러한 의존도 타파
2. 이제 소스 애플리케이션에서 생성되는 데이터는 어느 타깃 애플리케이션으로 보낼 것인지 고민하지 않고, 일단 카프카로 넣으면 됨
3. 카프카 내부에 데이터가 가저장되는 파티션 동작은 FIFO(First-In First-Out) 방식의 큐 자료구조와 유사 (컨슈머가 적재된 데이터를 가져가더라도, 파티션에 있는 데이터는 삭제되지 않음)
4. 큐에 데이터를 보내는 것이 '프로듀서(Producer)', 큐에서 데이터를 가져오는 것이 '컨슈머(Consumer)'
5. 토픽 (Topic) : RDBMS의 Table과 같은 개념
   - 구분하고자 하는 데이터 구분에 따라 토픽을 새로 만들고 운영
   - 토픽은 1개 이상 파티션을 가짐
6. 커밋 (Commit) : 특정 컨슈머가 어떤 데이터를 읽었는지 기록
